{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üåü **NLP ASSIGNMENT**\n",
        "### üìù**NLP Pipeline**\n",
        "#### üë©‚Äçüéì*Submitted by :* **Aruhi Choudhary**\n",
        "---\n"
      ],
      "metadata": {
        "id": "AlOdAy0npeNu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîπ **Part 1 ‚Äî NLP Pipeline**  \n",
        "In this section, we perform:  \n",
        "1Ô∏è‚É£ Tokenization  \n",
        "2Ô∏è‚É£ Stopword Removal  \n",
        "3Ô∏è‚É£ Stemming  \n",
        "4Ô∏è‚É£ POS Tagging  \n",
        "5Ô∏è‚É£ Lemmatization  \n",
        "\n",
        "Each step shows output clearly.\n"
      ],
      "metadata": {
        "id": "A62QCBqs6Xl6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üìå STEP 1 : Install and Imports Libraries"
      ],
      "metadata": {
        "id": "qKDWncnesdQ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*This cell installs required NLP libraries and imports all dependencies.*"
      ],
      "metadata": {
        "id": "xkvntVlmtzJU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q nltk\n",
        "\n",
        "import nltk\n",
        "\n",
        "# Required downloads\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# POS taggers (NEW requirement)\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "# Imports\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import wordnet\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XcgnLl_juA0i",
        "outputId": "067c31bf-746f-4aeb-8f74-d82e316af1f8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üìå STEP 2 : Paragraph Input"
      ],
      "metadata": {
        "id": "uqzE-f2GuLAA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*This cell stores a custom paragraph for applying NLP steps.*"
      ],
      "metadata": {
        "id": "Hq8rOlZUukzD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "paragraph = \"\"\"\n",
        "Natural Language Processing (NLP) is a branch of artificial intelligence\n",
        "that helps computers understand, interpret and generate human language.\n",
        "It is widely used in chatbots, sentiment analysis, translation and many real-world applications.\n",
        "\"\"\"\n",
        "\n",
        "print(\"Original Paragraph:\\n\")\n",
        "print(paragraph)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1xhqMx_uvgp",
        "outputId": "28612564-983e-4e65-b0bd-2e791bb10379"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Paragraph:\n",
            "\n",
            "\n",
            "Natural Language Processing (NLP) is a branch of artificial intelligence \n",
            "that helps computers understand, interpret and generate human language. \n",
            "It is widely used in chatbots, sentiment analysis, translation and many real-world applications.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üìå STEP 3 : Sentence Tokenization"
      ],
      "metadata": {
        "id": "fft2Rpmmvc0I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*This converts the paragraph into separate sentences.*"
      ],
      "metadata": {
        "id": "Idm2aMXXvuhF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sent_tokens = sent_tokenize(paragraph)\n",
        "\n",
        "print(\"\\n--- Sentence Tokenization ---\")\n",
        "print(sent_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkZV6bj6vwnw",
        "outputId": "06fa2da3-37c9-4ca7-f0a7-e8e4bf791ee4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Sentence Tokenization ---\n",
            "['\\nNatural Language Processing (NLP) is a branch of artificial intelligence \\nthat helps computers understand, interpret and generate human language.', 'It is widely used in chatbots, sentiment analysis, translation and many real-world applications.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üìå STEP 4 : Word Tokenization"
      ],
      "metadata": {
        "id": "FYB9Ow7-wGLJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*This splits the paragraph into individual words.*"
      ],
      "metadata": {
        "id": "3YqLTfnuxlyw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokens = word_tokenize(paragraph)\n",
        "\n",
        "print(\"\\n--- Word Tokenization ---\")\n",
        "print(word_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3sIU_JMxtuH",
        "outputId": "46131318-43aa-4a7c-d528-f4eae100bb9f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Word Tokenization ---\n",
            "['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'a', 'branch', 'of', 'artificial', 'intelligence', 'that', 'helps', 'computers', 'understand', ',', 'interpret', 'and', 'generate', 'human', 'language', '.', 'It', 'is', 'widely', 'used', 'in', 'chatbots', ',', 'sentiment', 'analysis', ',', 'translation', 'and', 'many', 'real-world', 'applications', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üìå STEP 5: Stopword Removal"
      ],
      "metadata": {
        "id": "EENm5XzyxxzI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*This removes common English stopwords like ‚Äúthe‚Äù, ‚Äúis‚Äù, ‚Äúand‚Äù, etc.*"
      ],
      "metadata": {
        "id": "9xeLmaROyMxV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "filtered_words = [\n",
        "    w for w in word_tokens\n",
        "    if w.lower() not in stop_words and w.isalnum()\n",
        "]\n",
        "\n",
        "print(\"\\n--- After Stopword Removal ---\")\n",
        "print(filtered_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qzU7Rn6ByTBK",
        "outputId": "8aa70b9f-a689-45fe-b9c0-78f1d80bce38"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- After Stopword Removal ---\n",
            "['Natural', 'Language', 'Processing', 'NLP', 'branch', 'artificial', 'intelligence', 'helps', 'computers', 'understand', 'interpret', 'generate', 'human', 'language', 'widely', 'used', 'chatbots', 'sentiment', 'analysis', 'translation', 'many', 'applications']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üìå STEP 6: Stemming"
      ],
      "metadata": {
        "id": "QjKEFLj2yfGR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*This reduces words to their root form using Porter Stemmer.*"
      ],
      "metadata": {
        "id": "AJ-shKyfyq0Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ps = PorterStemmer()\n",
        "\n",
        "stemmed_words = [ps.stem(w) for w in filtered_words]\n",
        "\n",
        "print(\"\\n--- After Stemming ---\")\n",
        "print(stemmed_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cp_QCWxqyxO1",
        "outputId": "e93031b3-b36c-444f-9072-c5a875f546d1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- After Stemming ---\n",
            "['natur', 'languag', 'process', 'nlp', 'branch', 'artifici', 'intellig', 'help', 'comput', 'understand', 'interpret', 'gener', 'human', 'languag', 'wide', 'use', 'chatbot', 'sentiment', 'analysi', 'translat', 'mani', 'applic']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üìå STEP 7: POS (Part-of-Speech) Tagging"
      ],
      "metadata": {
        "id": "P4q_vuK2y12h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*This assigns grammatical roles like noun, verb, adjective, etc.*"
      ],
      "metadata": {
        "id": "mYY-AAjazFN6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pos_tokens = pos_tag(filtered_words)\n",
        "\n",
        "print(\"\\n--- POS Tags ---\")\n",
        "print(pos_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNc8izMJzKcI",
        "outputId": "466c4b54-2445-4f71-b2d2-30eb05ffa8f1"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- POS Tags ---\n",
            "[('Natural', 'JJ'), ('Language', 'NNP'), ('Processing', 'NNP'), ('NLP', 'NNP'), ('branch', 'NN'), ('artificial', 'JJ'), ('intelligence', 'NN'), ('helps', 'VBZ'), ('computers', 'NNS'), ('understand', 'VBP'), ('interpret', 'JJ'), ('generate', 'NN'), ('human', 'JJ'), ('language', 'NN'), ('widely', 'RB'), ('used', 'VBN'), ('chatbots', 'NNS'), ('sentiment', 'JJ'), ('analysis', 'NN'), ('translation', 'NN'), ('many', 'JJ'), ('applications', 'NNS')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üìå STEP 8: Lemmatization"
      ],
      "metadata": {
        "id": "LHnZNiEpzk00"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*This converts words to proper dictionary/root form using POS tags.*"
      ],
      "metadata": {
        "id": "x_0NncSKz23s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Function to map POS tags to WordNet format\n",
        "def get_pos(tag):\n",
        "    if tag.startswith(\"J\"):\n",
        "        return wordnet.ADJ      # Adjective\n",
        "    elif tag.startswith(\"V\"):\n",
        "        return wordnet.VERB     # Verb\n",
        "    elif tag.startswith(\"N\"):\n",
        "        return wordnet.NOUN     # Noun\n",
        "    elif tag.startswith(\"R\"):\n",
        "        return wordnet.ADV      # Adverb\n",
        "    return wordnet.NOUN\n",
        "\n",
        "# Apply POS-aware Lemmatization\n",
        "lemmatized_words = [\n",
        "    lemmatizer.lemmatize(word, get_pos(tag))\n",
        "    for word, tag in pos_tokens\n",
        "]\n",
        "\n",
        "print(\"\\n--- After Lemmatization ---\")\n",
        "print(lemmatized_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jM67ZMz50AU4",
        "outputId": "a3e960c6-d2ff-4806-e96f-0859774f8d25"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- After Lemmatization ---\n",
            "['Natural', 'Language', 'Processing', 'NLP', 'branch', 'artificial', 'intelligence', 'help', 'computer', 'understand', 'interpret', 'generate', 'human', 'language', 'widely', 'use', 'chatbots', 'sentiment', 'analysis', 'translation', 'many', 'application']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üéâ Final Summary  \n",
        "| Step | Output Example | Description |\n",
        "|------|----------------|-------------|\n",
        "| Tokenization | ['Natural', 'Language', ...] | Converted text into tokens |\n",
        "| Stopword Removal | ['Natural','Language', ...] | Removed common words |\n",
        "| Stemming | ['natur', 'languag'] | Reduced words to base |\n",
        "| Lemmatization | ['natural', 'language'] | Dictionary form |\n",
        "\n",
        "‚û°Ô∏è This completes Part-1 of the assignment."
      ],
      "metadata": {
        "id": "4L3GBYUp64bu"
      }
    }
  ]
}